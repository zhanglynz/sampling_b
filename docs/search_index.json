[["index.html", "Uderstanding Survey Sampling Preface", " Uderstanding Survey Sampling Lingyun Zhang 2025-12-29 Preface How do you learn, or to be exact, how do you learn complex stuff? I guess that many different answers will come up, because we are all different in many aspects, such as culture background, knowledge level, and learning style etc. etc.. For myself, if I want to seriously learn some complex stuff, then I will write about it. Without doubts, sampling (or more exactly, survey sampling) is complex, and it is kind of boring1, but it is important (for me, because I need the knowledge/skills at my work, and for you the reasons may be different). So I have to learn and re-learn sampling, and this results in the current book. No matter what we study/learn, in my opinion—probably you agree with me—understanding is the most important goal that we want to achieve. For understanding, we must focus our attention on what, how and why, with unequal emphasis, depending on circumstances. A good starting point of learning about survey sampling is a couple of what questions: What is a sample? What is a good sample? We cover these questions especially the answers to them in Chapter 1. A little story told by Nan Laird in Ryan (2015): “We were informed that since it was Cochran’s last year of teaching at Harvard, we had to take all his courses straightaway. So we took Sample Surveys and Design of Experiments. I recall that the first day of class in Sample Surveys, Cochran stood at the board and said (in a kind of indistinct mumble) that this was the most boring topic in the world and he could not imagine why anyone would take the class.” For those who are interested in Cochran, please read this Wikipedia article.↩︎ "],["sample.html", "1 Understanding Sample 1.1 What is a sample? 1.2 What is a good sample? 1.3 On sampling error", " 1 Understanding Sample 1.1 What is a sample? The question in the section title is easy to answer: A sample is a subset of a population. Now, what is a population? Well, a population is defined in the planning stage of a study and is determined by study objectives. We can visualize a population using the following table: Member/Item/Unit ID Var\\(_1\\) Var\\(_2\\) \\(\\cdots\\) Var\\(_m\\) 1 2 \\(\\vdots\\) \\(N\\) So essentially a population is a list of members/items with associated characteristic variables that are of interest to the study. However, when it comes to the finer details of the population concept, things become more complicated. Following Lohr (2022), we distinguish three populations, namely, target population, sampling frame population and sampled population. We define target population; if we can create a sampling frame—this is often the case—then we have the sampling frame population; we can study/analyze sampled population, but we never sure about what exactly the sampled population is. In Figure 1, we draw a Venn diagram to show sampling frame population and target population. Figure 1: Two-population diagram. (Created by using R with some code borrowed from Peter Ellis, http://freerangestats.info/blog/2015/08/30/starting-in-datascience) Let’s have an example. Target population: all private dwellings in New Zealand. Sampling frame population: address list purchased from New Zealand Post. Not eligible for survey: some non-private dwellings, e.g. a prison. Not included in sampling frame: some new private dwellings not included in NZ Post system. 1.2 What is a good sample? According to Lohr (2010, p. 3), “a good sample will be representative in the sense that characteristics of interest in the population can be estimated from the sample with a known degree of accuracy.” According to Lohr (2022, p. 3), “a sample is representative if it can be used to ‘reconstruct’ what the population looks like—and if we can provide an accurate assessment of how good that reconstruction is.” So a good sample should meet the following requirements: No serious over and under coverage issues, that is, the sampling frame population should be sufficiently close to the target population. No serious selection bias issue. Measurement and processing errors are negligible. If the “soup” (a metaphor for the population) is not well mixed before we take a sample from it, then the sample cannot be good. Even if the “soup” is well mixed, selection bias may still happen because of e.g. biased sampling frame or biased response. By a biased sampling frame, we mean for whatever reasons the sampling frame excludes a subset of the population, resulting in discrepancy between it and the target population. That’s why we are concerned about coverage issues—under coverage may cause serious selection bias. By biased response, we mean the responding sample is not representative. At survey operational level, we monitor response rate. If response rate is low, we worry that we may get biased response. If response rate is very low, say 20% or even lower, then we suspect that the sample is suffering response bias. We omit details about measurement/processing errors here and refer readers to Lohr (2010), but we want to emphasize a point: There are many practical factors to consider in any sample survey. 1.3 On sampling error In the last Section, we mentioned coverage issue, selection bias, measurement and processing issues—these are all in the scope of non-sampling error. The flip side of the coin is sampling error, which is resulted from difference between a sample and the population in terms of characteristic variables. Sampling error is unavoidable! Let’s have a toy example. The population is shown below. Unit_ID \\(Y\\) 1 6 2 7 3 8 4 1 5 5 We can list all the 10 possible samples of size 3 by collecting the ID’s: \\[ \\{1, 2, 3\\}, \\ \\{1, 2, 4\\}, \\ \\{1, 2, 5\\}, \\ \\{1, 3, 4\\}, \\ \\{1, 3, 5\\} \\] \\[ \\{1, 4, 5\\}, \\ \\{2, 3, 4\\}, \\ \\{2, 3, 5\\}, \\ \\{2, 4, 5\\}, \\ \\{3, 4, 5\\} \\] The mean of \\(Y\\) (or population mean) is \\[ \\mu = \\frac{6+7+8+1+5}{5}=\\frac{27}{5}. \\] The 10 sample means are: 7, 14/3, 6, 5, 19/3, 4, 16/3, 20/3, 13/3, and 14/3, respectively. When we use one of the sample means to estimate the population mean, we expect that our estimate is away from the truth—this shows sampling error. We often use mean square error (MSE) or standard mean square error (SMSE) to measure sampling error. \\[ \\hbox{MSE}=\\hbox{average of}\\ (\\hat{\\theta}-\\theta)^2\\ \\hbox{over all possible samples}, \\] where \\(\\theta\\) is the population parameter (e.g. population mean \\(\\mu\\)) and \\(\\hat{\\theta}\\) is a sample estimator (e.g. sample mean \\(\\hat{\\mu}\\)). \\[ \\hbox{SMSE}=\\sqrt{\\hbox{MSE}}. \\] It can be shown that \\[ \\hbox{MSE}= \\hbox{var}(\\hat{\\theta}) + \\left(\\hbox{bias}(\\hat{\\theta})\\right)^2, \\] where \\[ \\hbox{var}(\\hat{\\theta})=\\hbox{average of}\\ \\left(\\hat{\\theta}-\\hbox{avg}(\\hat{\\theta})\\right)^2\\ \\hbox{over all possible samples} \\] and \\[ \\hbox{bias}(\\hat{\\theta})=\\hbox{avg}(\\hat{\\theta})-\\theta, \\] in which \\[ \\hbox{avg}(\\hat{\\theta})=\\hbox{average of}\\ \\hat{\\theta}\\ \\hbox{over all possible samples.} \\] By the way, if \\[ \\hbox{bias}(\\hat{\\theta})=0 \\] then we say that \\(\\hat{\\theta}\\) is an unbiased estimator. For the toy example above, \\[ \\hbox{MSE}=\\frac{(7-27/5)^2+(14/3-27/5)^2+\\cdots+(14/3-27/5)^2}{10}=0.973, \\] and \\[ \\hbox{SMSE}=\\sqrt{\\hbox{MSE}}=0.987. \\] The MSE (or SMSE) is only useful in theoretical evaluation of a sampling method. In practice, people are concerned about \\(\\hbox{bias}(\\hat{\\theta})\\) and \\(\\hbox{var}(\\hat{\\theta})\\). So we must have good estimates of \\(\\hbox{bias}(\\hat{\\theta})\\) and \\(\\hbox{var}(\\hat{\\theta})\\). If we have sound reasons to believe that \\[ \\hbox{bias}(\\hat{\\theta})=0, \\] then we focus our attention on estimate of \\(\\hbox{var}(\\hat{\\theta})\\). A confidence interval can be constructed if \\(\\hat{\\theta}\\) value and estimate of \\(\\hbox{var}(\\hat{\\theta})\\) are ready. For the toy example, for each possible sample we calculate \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (y_i-\\bar{y})^2, \\] where \\(n=3\\), \\(\\{y_1, \\ldots, y_n\\}\\) consists of the sample and \\(\\bar{y}\\) is the sample mean (i.e. an observed \\(\\hat{\\mu}\\)). Let us use \\[ \\sqrt{\\left(1-\\frac{n}{N}\\right)\\frac{1}{n}s^2,} \\] where \\(N=5\\), to estimate \\(\\sqrt{\\hbox{var}(\\hat{\\mu})}\\); the estimate is denoted by \\(\\hat{\\sigma}\\). We set confidence interval using the following formula: \\[ (\\hat{\\mu} - 3\\times \\hat{\\sigma},\\ \\hat{\\mu} + 3\\times \\hat{\\sigma}); \\] we can find that nine out of the ten resulted confidence intervals do contain the truth \\(27/5\\). Table 1.1: Toy example – 10 possible samples and confidence intervals. y_bar s2 sigma lower_limit upper_limit indi 4.000000 7.000000 0.9660918 1.1017247 6.898275 1 4.333333 9.333333 1.1155467 0.9866932 7.679973 1 4.666667 12.333333 1.2823589 0.8195899 8.513744 1 4.666667 10.333333 1.1737878 1.1453033 8.188030 1 5.000000 13.000000 1.3165612 1.0503165 8.949684 1 5.333333 14.333333 1.3824294 1.1860451 9.480622 1 6.000000 1.000000 0.3651484 4.9045549 7.095445 1 6.333333 2.333333 0.5577734 4.6600133 8.006653 1 6.666667 2.333333 0.5577734 4.9933466 8.339987 1 7.000000 1.000000 0.3651484 5.9045549 8.095445 0 NB: Here is a quick explanation about “nine out of the ten resulted confidence intervals do contain the truth”. \\[ \\begin{array}{cl} &amp; \\hbox{Pr}(|\\hat{\\mu} - \\mu| &lt; 3\\hat{\\sigma})\\\\ \\approx &amp; \\hbox{Pr}(|t_2| &lt; 3)\\\\ = &amp; 0.905\\ \\hbox{(keeping three decimal places)} \\end{array} \\] where \\(t_2\\) is a random variable having \\(t\\) distribution with 2 degrees of freedom. "],["sampling.html", "2 Understanding Sampling 2.1 General ideas 2.2 Three theoretical principles 2.3 Sampling in practice 2.4 Exercises", " 2 Understanding Sampling 2.1 General ideas Throughout this chapter, sampling means probability sampling. Following (but not strictly) Tillé and Wilhelm (2017), the settings are as follows: The population is \\({\\cal P}=\\{1, 2, \\ldots, i, \\ldots, N\\}\\). A sample \\(s\\) is a subset of \\({\\cal P}\\). Note that \\(s\\) can be empty. There are \\(2^N\\) possible samples; denote the set of all possible samples by \\(\\Omega\\). A sampling design specifies a probability distribution \\(p(\\cdot)\\) over \\(\\Omega\\) such that \\[ p(s)\\ge 0\\ \\hbox{and}\\ \\sum_{s\\in \\Omega} p(s)=1. \\] Define \\[ \\pi_i = \\hbox{the probability of selecting unit}\\ i \\] and for \\(i\\neq j\\) \\[ \\pi_{ij} = \\hbox{the probability that both units}\\ i\\ \\hbox{and}\\ j\\ \\hbox{are selected in a sample.} \\] Then, \\[ \\pi_i = \\sum_{i \\in s}p(s)\\ \\hbox{and}\\ \\pi_{ij}=\\sum_{\\{i,\\ j\\}\\subset s}p(s). \\] Examples of sampling designs: Simple random sampling: \\[ p(s)=\\left\\{ \\begin{array}{ll} {N \\choose n}^{-1}, &amp; \\hbox{if}\\ s\\in S_n,\\\\ 0, &amp; \\hbox{otherwise}, \\end{array} \\right. \\] where \\(S_n= \\{s\\in \\Omega | s\\ \\hbox{contains}\\ n\\ \\hbox{units}\\}\\) and \\(n\\) is the sample size. For this sampling design, \\[ \\pi_i = \\frac{n}{N}\\ \\hbox{and}\\ \\pi_{ij}= \\frac{n(n-1)}{N(N-1)}. \\] The essential in the simple random sampling is that all possible subsets having \\(n\\) units are equally likely to be chosen. Stratified sampling: \\[ p(s)=\\left\\{ \\begin{array}{ll} \\prod_{h=1}^H{N_h \\choose n_h}^{-1}, &amp;\\hbox{if}\\ \\#(s\\bigcap {\\cal P}_h)=n_h\\ \\hbox{for}\\ h=1,\\ldots, H,\\\\ 0, &amp; \\hbox{otherwise}, \\end{array} \\right. \\] where the population \\({\\cal P}\\) is partitioned into \\(H\\) strata (the \\(i\\)th one is denoted by \\({\\cal P}_i\\)) and \\(\\# {\\cal P}_h\\)—the cardinality of \\({\\cal P}_h\\)—is equal to \\(N_h\\), for \\(h=1, \\ldots, H\\); \\(n_h\\) is the sample size for stratum \\({\\cal P}_h\\). For this sampling design, \\[ \\pi_i = \\frac{n_h}{N_h},\\ \\hbox{if}\\ i\\in {\\cal P}_h, \\] and \\[ \\pi_{ij}=\\left\\{ \\begin{array}{ll} \\frac{n_h(n_h-1)}{N_h(N_h-1)}, &amp; \\hbox{if}\\ i, j\\in {\\cal P}_h,\\\\ \\frac{n_gn_h}{N_gN_h}, &amp;\\hbox{if}\\ i \\in {\\cal P}_g,\\ j\\in {\\cal P}_h,\\ g\\neq h. \\end{array} \\right. \\] Bernoulli sampling: \\[ \\pi_i = p,\\ \\hbox{where}\\ 0 &lt; p &lt; 1,\\ \\hbox{for}\\ i=1, \\ldots, N; \\] sample \\(s\\) has units that are independently selected. Note that for Bernoulli sampling, sample size is a random variable and it follows the Binomial\\((N, p)\\) distribution. The mean of the sample size is equal to \\(Np\\). Obviously, \\[\\pi_{ij}=p^2\\ \\hbox{for}\\ i\\neq j.\\] All the possible samples in \\(\\Omega\\) can be classified into \\(N+1\\) classes using their cardinality (number of elements in a set), and we can check that \\[ \\begin{array}{cl} &amp;\\sum_{i=0,\\ \\#s = i}^Np(s)\\\\ =&amp;\\sum_{i=0}^N {N \\choose i}p^i(1-p)^{N-i}\\\\ =&amp;1. \\end{array} \\] Poisson sampling: The setting is the same as that in Bernoulli sampling except that the selection probabilities for units are not all equal. The distribution of the sample size is called Poisson binomial distribution. Conditional Poisson sampling: This design has prescribed unequal inclusion probabilities but results in samples with fixed sample size; see more details in Tillé and Wilhelm (2017). 2.2 Three theoretical principles Tillé and Wilhelm (2017) introduce three theoretical principles for sampling design. Randomization: a) make sure there are as many samples as possible, while meeting other constraints; b) select a sample at random. Overrepresentation: unequal inclusion probabilities often result in more efficient estimates, or in other words, we should “preferentially select units where the dispersion is larger.” Restriction: avoid bad samples, e.g. using auxiliary information to make sure the estimates from a sample approximately equal the known totals. That is, “samples that either nonpractical or known to be inaccurate are avoided.” We quote the following from Tillé and Wilhelm (2017). When auxiliary information is available, it is desired to include it in the sampling design in order to increase the precision of the estimates. A balanced sample is such that the estimated totals of the auxiliary variables are approximately equal to the true totals. At first glance, the principle of restriction seems to be in contradiction with the principle of randomization because it restricts the number of possible number of samples with nonnull probabilities. However, the possible number of samples is so large that, even with several constraints, the number of possible samples with nonnull probabilities can remain very large. 2.3 Sampling in practice 2.3.1 Systematic sampling As mentioned before, population and sample sizes are \\(N\\) and \\(n\\), respectively. The ideas in systematic sampling can be described as follows. The units in the population are put in some order, say, units \\(1,\\ 2,\\ \\ldots,\\ N\\). Randomly choose a unit as the start, e.g. start is unit \\(a\\), where \\(1\\le a\\le N\\). The jump number is \\(k\\), which is the greatest integer less than or equal to \\(N/n\\), i.e. \\(k = \\hbox{floor}(N/n)\\). Then the \\(n\\) units \\[a;\\ (a+k)\\ \\hbox{mod}\\ N;\\ \\ldots;\\ (a+(n-1)k)\\ \\hbox{mod}\\ N\\] are chosen as the sample. NB: If the result of the mod operation is 0, then unit \\(N\\) is chosen into sample. R program: sys_sampling &lt;- function(N, n, start = NULL, indi_output = FALSE) {# interval k &lt;- floor(N / n) # start if(is.null(start)) start &lt;- ceiling(runif(1, 0, 1) * N) if(!start %in% 1L:N) stop(&quot;&#39;start&#39; must be integer between 1 and N!&quot;) # sample the_sample &lt;- (start + (0:(n - 1)) * k) %% N the_sample &lt;- ifelse(the_sample == 0L, N, the_sample) the_sample &lt;- sort(the_sample) # output if(indi_output) { temp &lt;- 1L:N output &lt;- as.integer(temp %in% the_sample) return(output) } return(the_sample) } Example: Suppose \\(N=10\\) and \\(n=3\\). We can show all the possible samples resulted from systematic sampling. library(dplyr) library(knitr) library(kableExtra) a_tbl &lt;- bind_cols(lapply(1:10, function(s) sys_sampling(N = 10, n = 3, start = s))) col_names &lt;- paste0(&quot;s_&quot;, 1:10) names(a_tbl) &lt;- col_names kable(a_tbl, &quot;html&quot;, align = rep(&#39;c&#39;, 10)) %&gt;% kable_styling(full_width = F) s_1 s_2 s_3 s_4 s_5 s_6 s_7 s_8 s_9 s_10 1 2 3 4 1 2 3 1 2 3 4 5 6 7 5 6 7 4 5 6 7 8 9 10 8 9 10 8 9 10 It is easy to notice that each unit has selection probability \\[ \\pi = \\frac{n}{N}=\\frac{3}{10}. \\] In our example, while simple random sampling results in 120 possible samples of size 3, systematic sample can result in only 10 possible samples of size 3. Judged by Randomization principle, systematic sampling is worse than simple random sampling. However, systematic sampling is often used by practitioners because of its simplicity. 2.3.2 Probability proportional to size In a sampling design, an important part is to determine selection probabilities, which accompany the units in the population. In terms of selection probabilities, we can distinguish two cases. Case 1: all the selection probabilities are equal. Case 2: the selection probabilities are not equal. Simple random sampling is an example of Case 1; probability proportional to size (PPS) is a method for creating unequal selection probabilities, i.e. it’s Case 2. With PPS, the selection probability of unit \\(i\\) is defined as \\[\\begin{equation} \\pi_i = n \\frac{U_i}{\\sum_{i=1}^N U_i},\\ \\text{for}\\ i=1, \\ldots, N, \\tag{2.1} \\end{equation}\\] where \\(n\\) and \\(N\\) are the sample size and population size, respectively, \\(U_i\\) is the size/importance of unit \\(i\\). R program: pps_action &lt;- function(size_vec, the_n) {the_re &lt;- the_n * (size_vec / sum(size_vec)) the_n_fixed &lt;- the_n while(1) { bad_ones_index &lt;- which(the_re &gt; 1) good_ones_index &lt;- which(the_re &lt; 1) if(!length(bad_ones_index)) return(the_re) the_re[bad_ones_index] &lt;- 1 m &lt;- sum(the_re == 1) the_n &lt;- the_n_fixed - m the_re[good_ones_index] &lt;- the_n * (size_vec[good_ones_index] / sum(size_vec[good_ones_index])) } } Remarks: If some of \\(\\pi_i\\) resulted from (2.1) are greater than 1, then we set them as 1, and recalculate the rest \\(\\pi_i\\)’s. The iteration process finishes until no \\(\\pi\\)’s are greater than 1. It is easy to see from (2.1) that \\[\\begin{equation} \\sum_{i=1}^N \\pi_i = n. \\tag{2.2} \\end{equation}\\] Actually, (2.2) always holds true if the sample size is fixed as \\(n\\)—see in 2.4.4. 2.3.3 Systematic sampling with unequal selection probabilities The settings are: Population has \\(N\\) units, and they are referred to as units 1, 2, \\(\\ldots\\), \\(N\\). Unit \\(i\\) has selection probability \\(\\pi_i\\). Note that \\[ \\sum_{i=1}^N \\pi_i = n, \\] where \\(n\\) is the sample size. In Section 2.3.1, we study systematic sampling, where the selection probabilities are all equal. An interesting question is: If the selection probabilities are not equal, how do we do systematic sampling? According to Antoine (2015) and Tillé (2010), an algorithm is as follows: Generate a random number from Uniform(0, 1), say it is \\(u\\). Each unit in the population owns an interval, i.e. unit 1 owns interval \\((0,\\ \\pi_1)\\); unit 2 owns interval \\((\\pi_1,\\ \\sum_{i=1}^2 \\pi_i)\\); \\(\\cdots\\); unit \\(N\\) owns interval \\((\\sum_{i=1}^{N-1}\\pi_i,\\ n)\\). Then, we find the intervals that contain \\(u\\), or \\(u+1\\), \\(\\cdots\\), or \\(u+(n-1)\\), and the \\(n\\) units that own these intervals are selected as a sample. The following is an R program that implements the algorithm. sys_sampling_with_sel_prob &lt;- function(sel_prob) { u &lt;- runif(1) cum_sel_prob &lt;- cumsum(sel_prob) n &lt;- floor(sum(sel_prob)) thresholds &lt;- u + 0:(n - 1) thresholds &lt;- thresholds[thresholds &lt;= max(cum_sel_prob)] idx &lt;- findInterval(thresholds, cum_sel_prob) + 1 s &lt;- integer(length(sel_prob)) s[idx] &lt;- 1L return(s) } The R function UPsystematic() in sampling package can do systematic sampling with unequal probabilities. Below, we show an example to test sys_sampling_with_sel_prob() and sampling::UPsystematic(). library(sampling) library(dplyr) library(knitr) # for evaluation eval_sampling &lt;- function(it_nbr = 10000, selec_prob, the_func) {the_vec &lt;- unlist(lapply(1:it_nbr, function(x) which(the_func(selec_prob) == 1))) as.data.frame(table(the_vec)) %&gt;% mutate(relative_freq = Freq / it_nbr) %&gt;% mutate(the_pi = selec_prob) } N &lt;- 6 n &lt;- 3 selc_prob &lt;- 3 * (1:N)/sum(1:N) # Simple example - sample selection the_re_4_sys &lt;- eval_sampling(selec_prob = selc_prob, the_func = sys_sampling_with_sel_prob) kable(the_re_4_sys) the_vec Freq relative_freq the_pi 1 1436 0.1436 0.1428571 2 2854 0.2854 0.2857143 3 4268 0.4268 0.4285714 4 5732 0.5732 0.5714286 5 7146 0.7146 0.7142857 6 8564 0.8564 0.8571429 the_re_4_UPmaxentropy &lt;- eval_sampling(selec_prob = selc_prob, the_func = UPmaxentropy) kable(the_re_4_UPmaxentropy) the_vec Freq relative_freq the_pi 1 1412 0.1412 0.1428571 2 2796 0.2796 0.2857143 3 4301 0.4301 0.4285714 4 5738 0.5738 0.5714286 5 7162 0.7162 0.7142857 6 8591 0.8591 0.8571429 2.4 Exercises 2.4.1 Unit selection probability in SRS Show that in simple random sampling, each unit has a selection probability of \\(n/N\\), where \\(n\\) and \\(N\\) are sample and population sizes, respectively. Hint: \\[ {N-1 \\choose n-1}/{N \\choose n}=\\frac{n}{N}. \\] 2.4.2 All possible samples for SRS with replacement Suppose the population is \\({\\cal P}=\\{1,\\ 2,\\ 3,\\ 4,\\ 5\\}\\). If we do SRS with replacement, where \\(n=3\\). Write an R program to list all possible samples. Note that if the selection resulted in, e.g. 1, 1, 1, then the sample is {1}—i.e. we don’t keep duplicated units. Hint: There are \\[ {5 \\choose 1}+{5 \\choose 2}+{5 \\choose 3} = 5 + 10 + 10 = 25 \\] possible samples. 2.4.3 An algorithm for drawing an SRS In Lohr (2010, 2022), one algorithm for drawing an SRS (Simple Random Sample without replacement) is as follows. Produce \\(N\\) random numbers from the Uniform(0, 1) distribution, and attach them to the \\(N\\) units in the sampling frame. Order the \\(N\\) units by the generated random numbers from the smallest to largest. The top \\(n\\) units are selected as the sample. Show that, indeed, this is simple random sampling without replacement. Hint: We can show that each of the \\({N \\choose n}\\) possible samples of size \\(n\\) has probability \\[ \\frac{n!(N-n)!}{N!}, \\] which is \\(1/{N \\choose n}\\), of being selected. 2.4.4 Useful results Suppose the population size is \\(N\\). If the sample size is \\(n\\), which is fixed, then \\[ E(\\sum_{k = 1}^ n w_{k(s)} y_{k(s)}) = \\sum_{k=1}^N y_k, \\] where \\(w_{k(s)}\\) is the weight for the \\(k\\)-th unit in the sample (indicated by subscript \\((s)\\)) and \\(y_{k(s)}\\) is the \\(k\\)-th unit in the sample. Two special cases: Let \\(y_k\\) be \\(\\pi_k = 1/w_k\\), then we have \\[ \\sum_{k=1}^N \\pi_k = n. \\] Let \\(y_k = 1\\) for \\(k=1, \\ldots, N\\), then \\[ E(\\sum_{k=1}^n w_{k(s)}) = N. \\] "],["weighting-system.html", "3 Weighting System 3.1 General ideas about weighting 3.2 The three stages in producing final weights 3.3 Calibration", " 3 Weighting System The main reference for this chapter is Haziza and Beaumont (2017). 3.1 General ideas about weighting Since a sample is a part/portion of a population, each item/individual in the sample represents itself plus others in the population; that is, a weight \\(w_i\\) should be attached to the item/individual \\(i\\). It’s obvious that \\[w_i&gt;0\\] or sometimes \\[ w_i \\ge 1. \\] Weights are important because they are used in estimation of population parameters. Given the data, i.e. variable values and weights \\(y\\) weight \\(y_1\\) \\(w_1\\) \\(y_2\\) \\(w_2\\) \\(\\vdots\\) \\(\\vdots\\) \\(y_n\\) \\(w_n\\) we have estimates of the total and mean \\[\\begin{align} \\hat{t} &amp;= \\sum_{i=1}^n w_i y_i,\\\\ \\hat{\\bar{y}} &amp;= \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}. \\end{align}\\] In the literature, the estimator \\(\\hat{t}\\) is called the Horvitz-Thompson (HT) estimator. 3.2 The three stages in producing final weights Stage 1: design weights \\[ d_i = \\frac{1}{\\pi_i}, \\] where \\(\\pi_i\\) is the selection/inclusion probability for item/individual \\(i\\). Stage 2: weights adjusted for non-response. We use \\(\\tilde{d}_i\\) to denote the adjusted weight for item/individual \\(i\\). Stage 3: final weights. We use \\(w_i\\) to denote the final weight for item/individual \\(i\\). 3.3 Calibration Notation We denote a population by \\(U\\). A sample is denoted by \\(S\\). Design weights: \\(\\{d_k,\\ k\\in S\\}\\). Final weights: \\(\\{w_k,\\ k \\in S\\}\\). What does calibration mean? Roughly speaking, calibration is to adjust from the design weights \\(\\{d_i\\}\\) to final weights \\(\\{w_i\\}\\) such that \\(\\{w_i\\}\\) are as close to \\(\\{d_i\\}\\) as possible; \\(\\{w_i\\}\\) satisfy calibration constraints. We will explain the exact meanings of a. and b. shortly. Why do we do calibration? According to Haziza and Beaumont (2017): The reasons for using calibration are three-fold: to force consistency of certain survey estimates to known population quantities; to reduce nonsampling errors such as nonresponse errors and coverage errors; to improve the precision of estimates. How do we do calibration? It’s clear that calibration is an optimization problem, where the decision variables are \\(\\{w_i\\}\\). The objective function is \\[ \\sum_{k\\in S} d_k\\frac{G(w_k/d_k)}{q_k}, \\] where \\(G(\\cdot)\\) is referred as a distance function—it measures the distance between \\(\\{w_i\\}\\) and \\(\\{d_i\\}\\), “\\(q_k\\) is a scale factor indicating the importance of unit \\(k\\) in the distance calculation. In most practical situations \\(q_k\\) is set to 1.” We assume that: a) \\(G(w_k/d_k)\\ge 0\\); b) \\(G(1)=0\\); c) \\(G(\\cdot)\\) is differentiable; d) \\(G(\\cdot)\\) is strictly convex (i.e. the second derivative of \\(G\\) is positive.) The constraints are \\[ \\sum_{k \\in S} w_k \\mathbf x_k = \\mathbf t_{\\mathbf x}, \\] where \\[ \\mathbf x_k = (x_{1k}, \\ldots, x_{Jk}), \\] and \\[ \\mathbf t_{\\mathbf x}=(t_{x_1}, \\ldots, t_{x_J}), \\] with \\[ t_{x_j}=\\sum_{k \\in U} x_{jk}. \\] Example 1 Suppose that the population size \\(N\\) is known. Setting \\[ x_k =1\\ \\hbox{and}\\ q_k=q\\ \\hbox{for}\\ k\\in S, \\] we want to minimize \\[ f(w_k;\\ k\\in S)=\\sum_{k \\in S}d_k G(w_k/d_k), \\] subject to \\[ \\sum_{k \\in S}w_k= N. \\] It’s easy to derive that \\[ w_k = N\\frac{d_k}{\\sum_{i \\in S} d_i}. \\] In this case, the calibrated total estimator is \\[\\begin{align} \\hat{t}_{y,\\ C} &amp; = \\sum_{k \\in S}w_k y_k\\\\ &amp;= \\sum_{k \\in S} N \\frac{d_k}{\\sum_{i \\in S} d_i} y_k\\\\ &amp;= N\\frac{\\hat{t}_{y,\\ \\pi}}{\\hat{N}_{\\pi}}. \\end{align}\\] Example 2 Suppose \\(x_k\\) is available for all \\(k \\in S\\) and it’s known that \\(\\sum_{k \\in U} x_k= t_x\\). Setting \\(q_k=x_k^{-1}\\), we want to minimize \\[ f(w_k;\\ k\\in S)=\\sum_{k\\in S}d_k G(w_k/d_k) x_k, \\] subject to \\[ \\sum_{k\\in S}w_k x_k=t_x. \\] We can derive that \\[ w_k = d_k\\frac{t_x}{\\sum_{k \\in S}d_k x_k}=d_k\\frac{t_x}{\\hat{t}_{x,\\ \\pi}}. \\] In this case, the calibrated total estimator is \\[ \\hat{t}_{y,\\ C}=\\frac{\\hat{t}_{y,\\ \\pi}}{\\hat{t}_{x,\\pi }}t_x, \\] which is the so-called ratio estimator. Example 3 Let \\(N_M\\) and \\(N_F\\) denote the numbers of males and females, respectively, in the population. Assume that \\(N_M\\) and \\(N_F\\) are known. Divide \\(S\\) into \\(M\\) and \\(F\\), where \\(M\\) is the set of males and \\(F\\) is the set of females. We want to minimize \\[ f(w_k; k\\in S) = \\sum_{k \\in M}d_k G(w_k/d_k)q_M + \\sum_{k \\in F}d_k G(w_k/d_k)q_F, \\] subject to \\[ \\sum_{k \\in M}w_k= N_M\\ \\hbox{and}\\ \\sum_{k \\in F}w_k=N_F. \\] It’s easy to show that \\[ w_k = \\left\\{ \\begin{array}{cl} d_k\\frac{N_M}{\\sum_{k \\in M}d_k}, &amp; \\hbox{if}\\ k\\in M\\\\ d_k\\frac{N_F}{\\sum_{k \\in F}d_k}, &amp; \\hbox{if}\\ k\\in F. \\end{array} \\right. \\] In this case, the calibrated total estimator is \\[ \\hat{t}_{y,\\ c}=\\frac{N_M}{\\hat{N}_{M,\\ \\pi}}\\hat{t}_{My,\\ \\pi} + \\frac{N_F}{\\hat{N}_{F,\\ \\pi}}\\hat{t}_{Fy,\\ \\pi}, \\] where \\[\\begin{align} \\hat{N}_{M,\\ \\pi}&amp;=\\sum_{k\\in M} d_k,\\\\ \\hat{N}_{F,\\ \\pi}&amp;=\\sum_{k\\in F} d_k,\\\\ \\hat{t}_{My,\\ \\pi}&amp;=\\sum_{k \\in M}d_k y_k,\\\\ \\hat{t}_{Fy,\\ \\pi}&amp;=\\sum_{k \\in F}d_k y_k. \\end{align}\\] Notice that here we are talking about the so-called post-stratified estimator. Commonly used distance functions "],["estimation-with-survey-data.html", "4 Estimation With Survey Data 4.1 Estimating the total 4.2 Estimating the mean 4.3 Estimating quantiles", " 4 Estimation With Survey Data By survey data, we mean in the data table one column is for the variable of interest \\(Y\\) and another column is for weights \\(W\\). Let \\[ Y=(y_1, y_2, \\ldots, y_n)^T, \\] and \\[ W=(w_1, w_2, \\ldots, w_n)^T. \\] 4.1 Estimating the total This is the so-called HT (Horvitz and Thompson) estimator \\[ \\hat{T}= \\sum_{i=1}^n w_i y_i. \\] 4.2 Estimating the mean The Hájek estimator \\[ \\hat{\\mu}= \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}. \\] 4.3 Estimating quantiles Suppose we want to estimate the \\(10th\\), \\(50th\\) and \\(90th\\) percentiles. One idea is as follows: Turn \\((Y, W)\\) as a discrete distribution by normalizing the weights, that is, \\[ p_i = \\frac{w_i}{\\sum_{i=1}^n w_i}, \\ \\hbox{for}\\ i=1, \\ldots, n. \\] Take \\(N\\) (e.g. \\(N = 10^5\\)) i.i.d. values from the distribution \\[ \\left( \\begin{array}{cccc} y_1 &amp; y_2 &amp; \\cdots &amp; y_n\\\\ p_1 &amp; p_2 &amp; \\cdots &amp; p_n \\end{array} \\right). \\] Using the empirical distribution formed by the i.i.d. values to estimate quantiles. R program: estimate_percentile &lt;- function(y_vec, weight_vec, N = 100000, the_p = 50) {if(the_p &lt; 1e-6 || the_p &gt; 100) stop(&quot;the_p should be between 0 and 100!&quot;) # a helper function sample_discrete_v &lt;- function(support_vec, prob_vec, n) {cp &lt;- cumsum(prob_vec) rand_nbrs &lt;- runif(n, 0, 1) the_index &lt;- vapply(rand_nbrs, function(x) which(x &lt; cp)[1], FUN.VALUE = numeric(1)) support_vec[the_index] } p_vec &lt;- weight_vec / sum(weight_vec) a_sample &lt;- sample_discrete_v(support_vec = y_vec, prob_vec = p_vec, n = N) the_r &lt;- as.integer(the_p / 100 * N) sorted_ &lt;- sort(a_sample) the_re &lt;- sorted_[the_r] return(the_re) } # NB: A reference for generating random numbers from a discrete distribution # https://stats.stackexchange.com/questions/26858/how-to-generate-numbers-based-on-an-arbitrary-discrete-distribution "],["sampling-errors.html", "5 Sampling Errors", " 5 Sampling Errors "],["references.html", "References", " References Antoine, R. (2015). (https://stats.stackexchange.com/users/69902/antoine-r), Systematic Sampling With Unequal Probabilities, URL (version: 2015-02-27): https://stats.stackexchange.com/q/139568 Haziza, D. and Beaumont, J. (2017). Construction of Weights in Surveys: A Review. Statistical Science, Vol. 32, pp. 206-226. Lohr, S. (2010). Sampling: Design and Analysis. Second Edition. Lohr, S. (2022). Sampling: Design and Analysis. Third Edition. Chapman and Hall/CRC. Ryan, L. (2015). A Conversation with Nan Laird. Statistical Science, Vol. 30, pp. 582-596. Tillé, Y (2010). Algorithms of Sampling With Equal or Unequal Probabilities. Euskal Estatistika Erakundea XXIII Seminario, November 2010 http://www.eustat.es/productosServicios/52.1_Unequal_prob_sampling.pdf Tillé, Y. and Wilhelm, M. (2017). Probability Sampling Design: Principles for Choice of Design and Balancing. Statistical Science, Vol. 32, pp. 176-189. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
