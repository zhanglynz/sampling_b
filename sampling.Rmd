# Understanding Sampling {#sampling}

## General ideas

Throughout this chapter, *sampling* means *probability sampling*. Following (but not strictly) Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017), the settings are as follows:

- The *population* is ${\cal P}=\{1, 2, \ldots, i, \ldots, N\}$.
- A *sample* $s$ is a subset of ${\cal P}$. Note that $s$ can be empty.
- There are $2^N$ possible samples. Let the set of all the possible samples be denoted by $\Omega$.

A *sampling design* specifies a probability distribution $p(\cdot)$ over $\Omega$ such that
$$
p(s)\ge 0\ \hbox{and}\ \sum_{s\in \Omega} p(s)=1.
$$

Define 
$$
\pi_i = \hbox{the probability of selecting unit}\ i
$$
and for $i\neq j$
$$
\pi_{ij} = \hbox{the probability that both units}\ i\ \hbox{and}\ j\ \hbox{are selected in the sample.}
$$
Then,
$$
\pi_i = \sum_{i \in s}p(s)\ \hbox{and}\ \pi_{ij}=\sum_{\{i,\ j\}\subset s}p(s).
$$

Examples of sampling designs:

- **Simple random sampling:**
$$
p(s)=\left\{
\begin{array}{ll}
{N \choose n}^{-1}, & \hbox{if}\ s\in S_n,\\
0, & \hbox{otherwise},
\end{array}
\right.
$$
where $S_n= \{s\in \Omega|\#s = n\}$ and $n$ is the sample size. For this sampling design, 
$$
\pi_i = \frac{n}{N}\ \hbox{and}\ \pi_{ij}= \frac{n(n-1)}{N(N-1)}.
$$
The essential in simple random sampling is that **all possible subsets having $n$ units are equally likely to be chosen.**
- **Stratified sampling:**
$$
p(s)=\left\{
\begin{array}{ll}
\prod_{h=1}^H{N_h \choose n_h}^{-1}, &\hbox{if}\ \#(s\bigcap {\cal P}_h)=n_h\ \hbox{for}\ h=1,\ldots, H,\\
0, & \hbox{otherwise},
\end{array}
\right.
$$
where the population ${\cal P}$ is partitioned into $H$ strata and $\# {\cal P}_h= N_h$ for $h=1, \ldots, H$; $n_h$ is the sample size for stratum ${\cal P}_h$. For this sampling design,
$$
\pi_i = \frac{n_h}{N_h},\ \hbox{if}\ i\in {\cal P}_h,
$$
and
$$
\pi_{ij}=\left\{
\begin{array}{ll}
\frac{n_h(n_h-1)}{N_h(N_h-1)}, & \hbox{if}\ i, j\in {\cal P}_h,\\
\frac{n_gn_h}{N_gN_h}, &\hbox{if}\ i \in {\cal P}_g,\ j\in {\cal P}_h,\ g\neq h.
\end{array}
\right.
$$
- **Bernoulli sampling:**
$$
\pi_i = p,\ \hbox{where}\ 0 < p < 1,\ \hbox{for}\ i=1, \ldots, N;
$$ 
sample $s$ has units that are 
**independently** selected. Note that for Bernoulli sampling, *sample size* is a random variable and it follows
Binomial$(N, p)$. The mean of the sample size is equal to $Np$. Obviously, 
$$\pi_{i, j}=p^2\ \hbox{for}\ i\neq j.$$ All the possible samples in $\Omega$ can be classified into
$N+1$ classes using their *cardinality* (number of elements in a set), and we can check that 

$$
\begin{array}{cl}
 &\sum_{i=0,\ \#s = i}^Np(s)\\
=&\sum_{i=0}^N {N \choose i}p^i(1-p)^{N-i}\\
=&1.
\end{array}
$$ 

- **Poisson sampling:** 
The setting is the same as that in Bernoulli sampling except that the
selection probabilities for units are not all equal. The distribution of the sample size is
called *Poisson binomial distribution*.
- **Conditional Poisson sampling:** 
This design has prescribed **unequal inclusion probabilities** but results in samples with **fixed sample size**; see more details in Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017).

## Three theoretical principles

Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017)
introduce three theoretical principles for sampling design.

- **Randomization:** a) make sure there are as many samples as possible, while meeting other constraints; b) select a sample at random.

- **Overrepresentation:** unequal inclusion probabilities often result in more efficient estimates, or in other words, we should "preferentially select units where the dispersion is larger."

- **Restriction:** avoid bad samples, e.g. using auxiliary information to make sure the estimates from a sample approximately equal the known totals. That is, "samples that either nonpractical or known to be inaccurate are avoided."

We quote the following from Till`r knitr::asis_output("\u00E9")` and Wilhelm (2017).

> When auxiliary information is available, it is desired to include it in the sampling design in order to increase the precision of the estimates.
>
 
> A balanced sample is such that the estimated totals of the auxiliary variables are approximately equal to the true totals.
>

> At first glance, the principle of restriction seems to be in contradiction with the principle
of randomization because it restricts the number of possible number of samples with
nonnull probabilities. However, the possible number of samples is so large that, even with
several constraints, the number of possible samples with nonnull probabilities can remain very
large.
>

## Sampling in practice

### Systematic sampling {#sys1}

As mentioned before, population and sample sizes are $N$ and $n$, respectively.
The ideas in systematic sampling can be described as follows.

1. The units in the population are put in some order, say, units $1,\ 2,\ \ldots,\ N$.
1. Randomly choose a unit as the *start*, e.g. start is unit $a$, where $1\le a\le N$.
1. The *jump number* is $k$, which is the integer that is the closest to $N/n$. Then the $n$ units
$$a;\ (a+k)\ \hbox{mod}\ N;\ \ldots;\  (a+(n-1)k)\ \hbox{mod}\ N$$
are chosen as the sample. NB: If the result of the mod operation is 0, then unit $N$ is chosen into sample.

**R program:**
```{r}
sys_sampling <- function(N, n, start = NULL, indi_output = FALSE)
{# a helper function
 round_helper <- function(a)
 {L <- floor(a)
  U <- ceiling(a)
  d <- c(a - L, U - a)
  if(d[1] < d[2]) return(L)
  return(U)
 }  
 # interval
 k <- N / n
 k <- round_helper(k)
 # start
 if(is.null(start)) start <- ceiling(runif(1, 0, 1) * N)
 if(!start %in% 1L:N) stop("'start' must be integer between 1 and N!")
 # sample
 the_sample <- (start + (0:(n - 1)) * k) %% N 
 the_sample <- ifelse(the_sample == 0L, N, the_sample)
 the_sample <- sort(the_sample)
 # output
 if(indi_output) {
 temp <- 1L:N
 output <- as.integer(temp %in% the_sample)
 return(output) }
 return(the_sample)
}  
```


**Example:** Suppose $N=10$ and $n=3$. We can show all the possible samples resulted from systematic sampling.
```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
a_tbl <- bind_cols(lapply(1:10, 
                          function(s) sys_sampling(N = 10, n = 3, start = s))) 
col_names <- paste0("s_", 1:10)
names(a_tbl) <- col_names
kable(a_tbl, "html", align = rep('c', 10)) %>% 
  kable_styling(full_width = F)
```

It is easy to notice that each unit has selection probability 
$$
\pi = \frac{n}{N}=\frac{3}{10}.
$$

In our example, while simple random sampling results in 120 possible samples of size 3, systematic sample can result in only 10 possible samples of size 3. Judged by **Randomization principle**, systematic sampling is worse than simple random sampling. However, systematic sampling is often used by practitioners because of its simplicity.

### Probability proportional to size 

In a sampling design, an important part is to determine *selection probabilities*, which accompany the units in the population. In terms of selection probabilities, we can distinguish two cases. Case 1: all the selection probabilities are equal. Case 2: the selection probabilities are not equal. Simple random sampling is an example of Case 1; probability proportional to size (PPS) is a method for creating unequal selection probabilities, i.e. it's Case 2.

With PPS, the selection probability of unit $i$ is defined as
\begin{equation}
\pi_i = n \frac{Z_i}{\sum_{i=1}^N Z_i},\ \text{for}\ i=1, \ldots, N,
(\#eq:pi)
\end{equation}
where $n$ and $N$ are the sample size and population size, respectively, $Z_i$ is the *size/importance* of unit $i$.

**R program:**
```{r}
pps_action <- function(size_vec, the_n)
{the_re <- the_n * (size_vec / sum(size_vec))
 the_n_fixed <- the_n
 while(1) {
   bad_ones_index <- which(the_re > 1)
   good_ones_index <- which(the_re < 1)
   if(!length(bad_ones_index)) return(the_re)
   the_re[bad_ones_index] <- 1
   m <- sum(the_re == 1)
   the_n <- the_n_fixed - m
   the_re[good_ones_index] <-
     the_n * (size_vec[good_ones_index] / sum(size_vec[good_ones_index]))
 }
}
```


**Remarks:**

1. If some of $\pi_i$ resulted from \@ref(eq:pi) are greater than 1, then we set them as 1, and recalculate the rest $\pi_i$'s. The iteration process finishes until no $\pi$'s are greater than 1.  

2. It is easy to see from \@ref(eq:pi) that
\begin{equation}
\sum_{i=1}^N \pi_i = n.
(\#eq:pisum)
\end{equation}
Actually, \@ref(eq:pisum) is always hold if the sample size is fixed as $n$---see a proof in ???.
<!-- below is a quick proof: -->
<!-- \begin{align} -->
<!-- \sum_{i=1}^N \pi_i &= \sum_{i=1}^N\sum_{i\in s}p(s) \notag\\ -->
<!-- &= n\sum_{s\in \Omega}p(s) \notag\\ -->
<!-- &= n\times 1 \notag\\ -->
<!-- &= n. \notag -->
<!-- \end{align} -->


### Systematic sampling with unequal selection probabilities

The settings are:

- Population has $N$ units, and they are referred to as units 1, 2, $\ldots$, $N$.
- Unit $i$ has selection probability $\pi_i$.

Note that 
$$
\sum_{i=1}^N \pi_i = n,
$$
where $n$ is the sample size.

In Section \@ref(sys1), we study systematic sampling, where the selection probabilities are
all equal. An interesting question is: If the selection probabilities are not equal, how
do we do systematic sampling? According to Antoine (2015) and Till`r knitr::asis_output("\u00E9")` (2010), an algorithm is as follows:

1. Generate a random number from Uniform(0, 1), say it is $u$.
1. Each unit in the population owns an interval,  i.e. unit 1 owns interval $(0,\ \pi_1)$; unit 2 owns interval $(\pi_1,\ \sum_{i=1}^2 \pi_i)$; $\cdots$; unit $N$ owns interval $(\sum_{i=1}^{N-1}\pi_i,\ n)$. Then, we find the intervals that contain $u$, or $u+1$, $\cdots$, or $u+(n-1)$, and the $n$ units that own these intervals are selected as a sample.

The following is an R program that implements the algorithm.
```{r}
sys_smpling <- function(sel_prob)
{u <- runif(1, 0, 1)
 cum_sel_prob <- cumsum(sel_prob)
 m <- length(sel_prob)
 s <- rep(0L, m)
 for(i in 1:m)
 {s[i] <- as.integer(u <= cum_sel_prob[i])
  if(s[i] == 1L) u <- u + 1
 }
 return(s)
}
```

The R function `UPsystematic()` in **`sampling`** package can do systematic sampling with 
unequal probabilities. Below, we show an example to test `sys_smpling()` and `sampling::UPsystematic()`.
```{r, warning = FALSE, message = FALSE}
library(sampling)
library(dplyr)
library(knitr)

# for evaluation
eval_sampling <- function(it_nbr = 10000, selec_prob, the_func)
{the_vec <- unlist(lapply(1:it_nbr, 
                          function(x) which(the_func(selec_prob) == 1)))
 as.data.frame(table(the_vec)) %>% 
   mutate(relative_freq = Freq / it_nbr) %>% 
   mutate(the_pi = selec_prob)
}

N <-  6
n <-  3
selc_prob <- 3 * (1:N)/sum(1:N) 

# Simple example - sample selection 
the_re_4_sys <- eval_sampling(selec_prob = selc_prob, 
                              the_func = sys_smpling)
kable(the_re_4_sys)
the_re_4_UPmaxentropy <- eval_sampling(selec_prob = selc_prob, 
                                       the_func = UPmaxentropy)
kable(the_re_4_UPmaxentropy)
```




## Exercises

### Unit selection probability in SRS

Show that in simple random sampling, each unit has a selection probability of $n/N$, where $n$ and $N$ are sample and population sizes, respectively.

**Hint:** 
$$
{N-1 \choose n-1}/{N \choose n}=\frac{n}{N}.
$$

### All possible samples for SRS with replacement

Suppose the population is ${\cal P}=\{1,\ 2,\ 3,\ 4,\ 5\}$. If we do SRS
with replacement, where $n=3$. Write an R program to list all possible samples. Note that if the selection resulted in, e.g. 1, 1, 1, then the sample is {1}---i.e. we don't keep duplicated units. 

**Hint:** There are 
$$
{5 \choose 1}+{5 \choose 2}+{5 \choose 3} = 5 + 10 + 10 = 25
$$
possible samples.


### An algorithm for drawing an SRS 

In Lohr (2010, 2022), one algorithm for drawing an SRS (Simple Random Sample without replacement) is as follows.

1. Produce $N$ random numbers from the Uniform(0, 1) distribution, and attach them to the $N$ units in the sampling frame.
1. Order the $N$ units by the generated random numbers from the smallest to largest.
1. The top $n$ units are selected as the sample.

Show that, indeed, this is simple random sampling without replacement.

**Hint:** We can show that each of the ${N \choose n}$ possible samples of size $n$ has probability 
$$
\frac{n!(N-n)!}{N!},
$$
which is $1/{N \choose n}$, of being selected.

